{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IMHvFKpvsWB"
      },
      "source": [
        "# 1. Install and import packages\n",
        "\n",
        "* 본 실습에 필요한 패키지를 설치합니다.\n",
        "* 이번 실습에서는 SKT 에서 배포한 KoBERT를 사용합니다. https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcaJbr2whmjM",
        "outputId": "568b38d4-aee3-493c-fd30-d3810ee286ab"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install pandas tqdm\n",
        "!pip install gluonnlp==0.9.2\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.27.1)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gluonnlp==0.9.2 in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.2) (1.22.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.2) (0.29.34)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.2) (23.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-4gqkptt7/kobert-tokenizer_f798fd33829e47c58c2b0f09f2ba6297\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-4gqkptt7/kobert-tokenizer_f798fd33829e47c58c2b0f09f2ba6297\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZAt8H1uiz7Q"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import argparse\n",
        "from argparse import Namespace\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from transformers import (AutoTokenizer, AutoConfig, BertPreTrainedModel, BertModel, \n",
        "                          AdamW, get_linear_schedule_with_warmup)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5g0Nv-FlDQI"
      },
      "source": [
        "# 2. 데이터 다운로드 받기\n",
        "네이버 영화리뷰 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4HxZ6aj2J2",
        "outputId": "86a1b2f9-f75d-4a50-8a74-69f524c23b42"
      },
      "source": [
        "# 아래 코드 실행시 nsmc 디렉토리가 생성되어야합니다.\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nsmc' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4nMiFU9qqhIe",
        "outputId": "7539e968-d54b-45ab-818c-ab17af67c1a7"
      },
      "source": [
        "# Raw Data Exploration\n",
        "raw_data = open('./nsmc/ratings_train.txt').readlines()\n",
        "raw_data = [ele.strip().split(\"\\t\") for ele in raw_data]\n",
        "pd.DataFrame(raw_data).head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0                                  1      2\n",
              "0        id                           document  label\n",
              "1   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "2   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "3  10265843                  너무재밓었다그래서보는것을추천한다      0\n",
              "4   9045019      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90c73426-47be-4e35-9c79-d17e3f502d69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id</td>\n",
              "      <td>document</td>\n",
              "      <td>label</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90c73426-47be-4e35-9c79-d17e3f502d69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90c73426-47be-4e35-9c79-d17e3f502d69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90c73426-47be-4e35-9c79-d17e3f502d69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMeyx8ivkLBq"
      },
      "source": [
        "# import gluonnlp as nlp 라이브러리 사용\n",
        "train_data = nlp.data.TSVDataset(\"./nsmc/ratings_train.txt\", field_indices=[1,2],  num_discard_samples=1)\n",
        "test_data = nlp.data.TSVDataset(\"./nsmc/ratings_test.txt\", field_indices=[1,2],  num_discard_samples=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL5x7ENukcO9",
        "outputId": "154fe248-6c8e-4835-802d-d95aa3013eeb"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['아 더빙.. 진짜 짜증나네요 목소리', '0'],\n",
              " ['흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'],\n",
              " ['너무재밓었다그래서보는것을추천한다', '0'],\n",
              " ['교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '0'],\n",
              " ['사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '1']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH4jHaeAkdem",
        "outputId": "1c3ace0a-5e98-46ca-ed8d-1d49ee641adf"
      },
      "source": [
        "test_data[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['굳 ㅋ', '1'],\n",
              " ['GDNTOPCLASSINTHECLUB', '0'],\n",
              " ['뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아', '0'],\n",
              " ['지루하지는 않은데 완전 막장임... 돈주고 보기에는....', '0'],\n",
              " ['3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??', '0']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2yqDl_4pPzj"
      },
      "source": [
        "# 3. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQolZqWE8wwf"
      },
      "source": [
        "## 3.1. 토크나이저 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxO3m6UVmOED",
        "outputId": "8d1f6778-d132-4898-9d0c-9570860e01ad"
      },
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMW3SjG1mPe8",
        "outputId": "ee8f3b07-8bcb-458d-bc4b-2d845eb74d89"
      },
      "source": [
        "idx = 200\n",
        "print(train_data[idx][0])\n",
        "print(tokenizer.encode(train_data[idx][0]))\n",
        "print(tokenizer.decode(tokenizer.encode(train_data[idx][0])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TV용 건담 시리즈 중에서 아직까지도 최고봉\n",
            "[2, 694, 7003, 881, 5798, 2973, 4257, 6903, 3129, 5592, 5859, 4522, 6392, 3]\n",
            "[CLS] TV용 건담 시리즈 중에서 아직까지도 최고봉[SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb-eKncX2W96",
        "outputId": "46a66118-c618-492f-d93e-2218d747bd9c"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids('[CLS]')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3dLkogSpv5k"
      },
      "source": [
        "## 3.2. 데이터셋 생성하기\n",
        "pytorch 에서 제공하는 Dataset 클래스를 상속받아서 데이터셋 클래스를 생성합니다.\n",
        "\n",
        "반드시 \\__getitem__(self) 과 \\__len__(self) 을 오버로딩해야합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AXWxwi-0Jsv"
      },
      "source": [
        "def pad_ids(arrays, padding, max_length=-1):\n",
        "    if max_length < 0:\n",
        "        max_length = max(list(map(len, arrays)))\n",
        "    arrays = [\n",
        "        array + [padding] * (max_length - len(array))\n",
        "        for array in arrays\n",
        "    ]\n",
        "    return arrays"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fVPeawcmRBq"
      },
      "source": [
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentences = [ele[0] for ele in dataset]\n",
        "        self.labels = [ele[1] for ele in dataset]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        inputs = self.tokenizer.encode_plus(review)\n",
        "        return {\"inputs\": inputs[\"input_ids\"],\n",
        "                \"inputs_mask\": inputs[\"attention_mask\"],\n",
        "                \"targets\": label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        input_ids = [ins[\"inputs\"] for ins in batch]\n",
        "        input_mask = [ins[\"inputs_mask\"] for ins in batch]\n",
        "        targets = [int(ins[\"targets\"]) for ins in batch]\n",
        "\n",
        "        # batch 안의 데이터가 모든 같은 길이의 텐서가 될 수 있도록 작업\n",
        "        input_ids = torch.tensor(pad_ids(input_ids, self.tokenizer.pad_token_id), dtype=torch.long)\n",
        "        input_mask = torch.tensor(pad_ids(input_mask, 0), dtype=torch.long)\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        return {\"input_ids\": input_ids,\n",
        "                \"input_mask\": input_mask,\n",
        "                \"targets\": targets}\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_J2iVV7Cl-d",
        "outputId": "50725738-0c6b-4e9c-f3e2-0454f9012421"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available objects for config:\n",
            "     AliasManager\n",
            "     ColabHistoryManager\n",
            "     DisplayFormatter\n",
            "     IPCompleter\n",
            "     IPKernelApp\n",
            "     InlineBackend\n",
            "     LoggingMagics\n",
            "     MagicsManager\n",
            "     OSMagics\n",
            "     PrefilterManager\n",
            "     ScriptMagics\n",
            "     Shell\n",
            "     StoreMagics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUd-4tNptPgH"
      },
      "source": [
        "# 4. 모델 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ1mKlBtN1q"
      },
      "source": [
        "class KoBERTClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(KoBERTClassifier, self).__init__(config, args)\n",
        "        config.num_labels = 2\n",
        "        self.config = config\n",
        "        self.args = args\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        ################## TODO 1 ###########################\n",
        "        # Sentiment 를 분류하는 linear layer 를 선언\n",
        "        # layer 이름은 classifier\n",
        "        self.classifier = nn.Linear(config.hidden_size,2)\n",
        "        #hidden size 768\n",
        "        ####################################################\n",
        "        \n",
        "        ################## TODO 2 ###########################\n",
        "        # loss funtion을 Cross Entropy Loss 로 설정\n",
        "        # 변수명은 loss_fn\n",
        "        self.loss_fn = CrossEntropyLoss()\n",
        "        ####################################################\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, targets):\n",
        "\n",
        "        ################## TODO 3 ###########################\n",
        "        # bert 모델에 input 넣기\n",
        "        output = self.bert(input_ids, attention_mask)\n",
        "\n",
        "        ####################################################\n",
        "\n",
        "        pool_output = output[1]\n",
        "        cls_output = self.classifier(pool_output)\n",
        "        loss = self.loss_fn(cls_output, targets)\n",
        "\n",
        "        return (loss, cls_output)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVPtlcMvqU3"
      },
      "source": [
        "# 5. 학습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9_eXR_zl3p"
      },
      "source": [
        "## 5.1. 파라미터 셋업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxe5QXqsu52B"
      },
      "source": [
        "args = Namespace()\n",
        "args.train_batch_size = 32\n",
        "args.eval_batch_size = 32\n",
        "args.num_train_epochs = 6\n",
        "args.learning_rate = 2e-5\n",
        "args.gradient_accumulation_steps = 1\n",
        "args.warmup_steps = 0\n",
        "args.weight_decay = 0.0\n",
        "args.adam_epsilon = 1e-8\n",
        "args.max_grad_norm = 1.0"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8umpkNFoT1Ue"
      },
      "source": [
        "## 5.2. 데이터셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijXY37eUvZQP",
        "outputId": "b7b22158-00f6-4718-c4b8-0db919fb771b"
      },
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH0P8FNaqWBV"
      },
      "source": [
        "# 학습데이터셋 생성\n",
        "train_dataset = ReviewDataset(train_data[:10000], tokenizer)\n",
        "test_dataset = ReviewDataset(test_data[:1000], tokenizer)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5Zhuj65o2idv",
        "outputId": "01cbaac9-f237-4b7f-8214-6b07a670e938"
      },
      "source": [
        "train_dataset.sentences[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'아 더빙.. 진짜 짜증나네요 목소리'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ntr7dtcyjYK",
        "outputId": "16aa6e0d-788d-412f-fadb-a86e556332e3"
      },
      "source": [
        "print(train_dataset[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'inputs': [2, 3093, 1698, 6456, 54, 54, 4368, 4396, 7316, 5655, 5703, 2073, 3], 'inputs_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'targets': '0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Z-Y2DZqV-v"
      },
      "source": [
        "# Dataloader: 학습 진행시 모델에 batch 단위로 데이터를 입력시키는 객체\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=args.train_batch_size, \n",
        "                                               collate_fn=train_dataset.collate_fn)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                              batch_size=args.eval_batch_size, \n",
        "                                              collate_fn=test_dataset.collate_fn)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucv2VqXnT5xW"
      },
      "source": [
        "## 5.3. 모델 준비\n",
        "model 을 cuda 로 올립니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_shAjDMWvy0l"
      },
      "source": [
        "# GPU 셋팅\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkxkXV4iyMXj"
      },
      "source": [
        "config = AutoConfig.from_pretrained(\"skt/kobert-base-v1\")\n",
        "model = KoBERTClassifier(config, args).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOzHrvxS5jC2"
      },
      "source": [
        "## 5.4. train, evaluate 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyGMsMQu45Dz"
      },
      "source": [
        "def train(args, model, train_iterator, eval_iterator):\n",
        "\n",
        "    t_total = len(train_iterator) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    for epoch in range(int(args.num_train_epochs)):\n",
        "        tr_loss = 0\n",
        "        model.zero_grad()\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(tqdm(train_iterator)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            input_mask = batch[\"input_mask\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "\n",
        "            ################## TODO 1 ###########################\n",
        "            # 1. GPU 에 올린 데이터를 모델에 넣어서 결과를 받아오기.\n",
        "            #     (Hint: 모델이 출력하는 것은 두개인데 학습 과정에서는 첫번째 항목이 매우 중요)\n",
        "            # 2. 모델이 출력한 첫번째 항목으로 model weight 의 gradient 계산\n",
        "            loss, _ = model(input_ids,input_mask,targets)\n",
        "            loss.backward()\n",
        "\n",
        "            ####################################################\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            ################## TODO 2 ###########################\n",
        "            # model weight, learning rate 를 업데이트\n",
        "            # 누적된 gradient 초기화\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            ####################################################\n",
        "\n",
        "\n",
        "        tr_loss = tr_loss / len(train_iterator)\n",
        "\n",
        "        eval_acc, eval_loss = evaluate(model, eval_iterator)\n",
        "        print()\n",
        "        print(f\"Epoch: {epoch}, Accuracy: {eval_acc}, Train_loss: {tr_loss}, Eval_loss: {eval_loss}\")\n",
        "    \n",
        "    return tr_loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOVzAtmz45Be"
      },
      "source": [
        "def calculate_accuracy(preds, y):\n",
        "\n",
        "    ################## TODO ###########################\n",
        "    # accuracy 를 계산하는 코드 짜기\n",
        "    # pred 중 가장 큰 값의 index가 모델이 분류한 class\n",
        "    max_idx = np.argmax(preds, axis=1)\n",
        "    correct = (max_idx == y)\n",
        "    acc=np.sum(correct)/len(correct)\n",
        "    ####################################################\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUgsB6E9qV0-"
      },
      "source": [
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    labels = []\n",
        "    preds = []\n",
        "    eval_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            input_mask = batch[\"input_mask\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "\n",
        "            ################## TODO ###########################\n",
        "            # 1. GPU 에 올린 데이터를 모델에 넣어서 결과를 받아오기\n",
        "            #     (Hint: 이번에는 모델의 output도 중요)\n",
        "            ####################################################\n",
        "            loss,logits = model(input_ids,input_mask,targets)\n",
        "\n",
        "            labels.append(targets.detach().cpu().numpy())\n",
        "            preds.append(logits.detach().cpu().numpy())\n",
        "            eval_loss += loss.item()\n",
        "    \n",
        "    labels = np.concatenate(labels)\n",
        "    preds = np.concatenate(preds)\n",
        "    acc = calculate_accuracy(preds, labels)\n",
        "    eval_loss = eval_loss / len(iterator)\n",
        "\n",
        "    return acc, eval_loss\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수업 시간에 도대체 왜 이런지는 모르겠으나 "
      ],
      "metadata": {
        "id": "9gwEuLaq7FJ2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WDNV4KG5OLe",
        "outputId": "c4dd2bec-37a0-4831-fd1e-429a842dcbfb"
      },
      "source": [
        "train_loss = train(args, model, train_dataloader, test_dataloader) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 313/313 [02:10<00:00,  2.39it/s]\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0, Accuracy: 0.744, Train_loss: 0.5254107715127567, Eval_loss: 0.5116545129567385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [02:09<00:00,  2.42it/s]\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Accuracy: 0.748, Train_loss: 0.4470973160510627, Eval_loss: 0.513939468190074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [02:09<00:00,  2.41it/s]\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, Accuracy: 0.755, Train_loss: 0.3852033080717626, Eval_loss: 0.5038031516596675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [02:09<00:00,  2.42it/s]\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3, Accuracy: 0.762, Train_loss: 0.357389184327933, Eval_loss: 0.526361346244812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [02:09<00:00,  2.41it/s]\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 4, Accuracy: 0.756, Train_loss: 0.3284704387664033, Eval_loss: 0.5575701505877078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [02:09<00:00,  2.41it/s]\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 5, Accuracy: 0.778, Train_loss: 0.2968378942400312, Eval_loss: 0.5658141542226076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE8cufBc5vue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944c67bc-e640-419b-8320-4fa89ab865e1"
      },
      "source": [
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        input_mask = batch[\"input_mask\"].to(device)\n",
        "        targets = torch.tensor(batch[\"targets\"]).to(device)\n",
        "\n",
        "        _, logits = model(input_ids=input_ids, attention_mask=input_mask, targets=targets)\n",
        "\n",
        "        preds.append(logits.detach().cpu().numpy())\n",
        "\n",
        "preds = np.concatenate(preds)\n",
        "test_res = np.argmax(preds, axis=1)\n",
        "test_res[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]<ipython-input-31-a0856e0bbcc6>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(batch[\"targets\"]).to(device)\n",
            "100%|██████████| 32/32 [00:04<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xoPZ7jg7C7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}