{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwlPAA8ZJUsr"
      },
      "source": [
        "\n",
        "Copyright (C) 2020-2023 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
        "\n",
        "you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software \n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki_RHIwPJvyn"
      },
      "source": [
        "# **1. TensorFlow Operations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5N1npMVQqJz"
      },
      "source": [
        "## Constant Tensor\n",
        "\n",
        "Let's create a constant tensor in TensorFlow.\n",
        "\n",
        "**```tf.constant(\n",
        "    value, dtype=None, shape=None, name='Const'\n",
        ")```**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFZ3bfVsQz_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f543281d-3fe6-452f-dfcb-8b0206a3665b"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# constant of 1d tensor, or a vector\n",
        "a = tf.constant([2,2], name = 'vector')\n",
        "\n",
        "# constant of 2x2 tensor, or a matrix\n",
        "b = tf.constant([[0,2], [1,3]], name = 'matrix')#tensor의 이름을 지어주는 형태로 되어져 있다 이 말이다.\n",
        "\n",
        "print(a.numpy())#텐서플로우는 그래프를 정의하고 실행하겠다 하면 실행을 하는 방식이고\n",
        "print(b.numpy())#위에서 numpy로 메소드를 부르면 시스템 속에서 이미 정의된 그래프를 실제로 구현하는 것이다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2]\n",
            "[[0 2]\n",
            " [1 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrE4WkZNUn9o"
      },
      "source": [
        "## Mathematical Operations\n",
        "\n",
        "The following example shows a matrix division operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drceRvn4VGec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316d5a7f-f053-4097-aa78-6959424c8f51"
      },
      "source": [
        "# Create constant tensors a and b\n",
        "a = tf.constant([2,4], name = 'a', dtype = tf.float32)\n",
        "b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
        "print(a.numpy())\n",
        "print(b.numpy())#텐서를 만들어 놓는 것이다.\n",
        "  \n",
        "# Execute division operation using b and a\n",
        "div = tf.divide(b, a)# or equivalently, div = b / a\n",
        "#broadcast 연산을 통해서 계산을 해주는 것이다.\n",
        "\n",
        "print('\\nPrint div')\n",
        "print(div.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 4.]\n",
            "[[0. 1.]\n",
            " [2. 3.]]\n",
            "\n",
            "Print div\n",
            "[[0.   0.25]\n",
            " [1.   0.75]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnNhG69dgWsA"
      },
      "source": [
        "## Quiz 1\n",
        "**Create two constants with shape=[2,2] and perform matrix multiplication. (HINT: use `tf.matmul`)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZyw665-gWsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e4d11c-8899-46b5-831c-20c0e506a8ba"
      },
      "source": [
        "import tensorflow as tf\n",
        "x = [[1, 2], [3, 4]]\n",
        "y = [[5, 6], [7, 8]]\n",
        "z = tf.matmul(x,y)\n",
        "print(z.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4UKKwwsZa4I"
      },
      "source": [
        "## Variables\n",
        "\n",
        "Shared, mutable states (e.g., model parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TubsFPeZ0Rz"
      },
      "source": [
        "### Creating Variable\n",
        "\n",
        "To declare a variable, you create an instance of the class `tf.Variable`.\n",
        "\n",
        "#### Usage of TF Variable\n",
        "\n",
        "\n",
        "```\n",
        "x = tf.Variable(...)\n",
        "x.read_value()      # read value\n",
        "x.assign(...)       # x = ...\n",
        "x.assign_add(...)   # x += ...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcriAnGbaCKk"
      },
      "source": [
        "One way to create a variable is: \n",
        "\n",
        "**```tf.Variable(< initial-value >, name = < optional-name >)```**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsaTk0M3hr0p"
      },
      "source": [
        "This example creates three variables using `tf.Variable`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32t-sxgsaFXh"
      },
      "source": [
        "# Create scalar variable\n",
        "s = tf.Variable(2, name = 'scalar')\n",
        "# Create matrix variable\n",
        "m = tf.Variable([[0,1], [2,3]], name = 'matrix')\n",
        "# Create zero matrix using tf.zeros\n",
        "W = tf.Variable(tf.zeros((784,10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O91x5GeZzW2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79692b49-0dd3-429a-f8b0-90a0310e0f0b"
      },
      "source": [
        "# Print values of Variable s, m and W\n",
        "print('s:')\n",
        "print(s.read_value().numpy())\n",
        "print('\\nm:')\n",
        "print(m.read_value().numpy())\n",
        "print('\\nW:')\n",
        "print(W.read_value().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s:\n",
            "2\n",
            "\n",
            "m:\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "W:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "239nTB8ScEMD"
      },
      "source": [
        "### Changing values of variables\n",
        "\n",
        "To change the value of a variable, we need to assign a new value to the variable.\n",
        "You can see variable `v` changes after `assign` operations are executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezu6kFnScVd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139740d2-7d52-4801-c5cc-bbb6523be895"
      },
      "source": [
        "# v is a 2 x 3 variable of random values\n",
        "initializer = tf.random_normal_initializer(mean=1., stddev=2.)#정규분포를 따르는 임의의 값을 뽑아내는 것이다.\n",
        "v = tf.Variable(initializer(shape=[2, 3]))\n",
        "\n",
        "# c is a 2 x 3 constant with 1.0\n",
        "c = tf.constant(1.0, shape=(2,3))\n",
        "\n",
        "# Get value\n",
        "print('v:')\n",
        "print(v.read_value().numpy())\n",
        "\n",
        "# Assign new value to the variable 새로운 값으로 갱신하기\n",
        "v.assign(c)\n",
        "\n",
        "# Get value again\n",
        "print('v:')\n",
        "print(v.read_value().numpy())\n",
        "\n",
        "# Assign new value to the variable\n",
        "v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "# Get value again\n",
        "print('v:')\n",
        "print(v.read_value().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v:\n",
            "[[-0.30118573  1.8997161   3.5982037 ]\n",
            " [ 2.1961584  -3.0604954   1.4173982 ]]\n",
            "v:\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "v:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3yizqwmxHs"
      },
      "source": [
        "## Quiz 2\n",
        "Define a variable (name : \"term\") with shape = [] and dtype = `tf.float64`. Initialize the variable as `2` first.\n",
        "Define another variable (name : \"sum\") with shape = [] and dtype = `tf.float64`. Initialize the variable as zeros. (remember: shape = [] does not mean wrapping the initial value with [])\n",
        "\n",
        "By using these two variables, compute the following:\n",
        "$sum = 1/term_1 + 1/term_2 + ... + 1/term_{10}$\n",
        "where\n",
        "$term_i = term_{i-1} * (term_{i-1} - 1) + 1$\n",
        "and $term_1 = 2$.\n",
        "(This recurrence relation is known as Sylvester's sequence.)\n",
        "\n",
        "Hint: Repeat updating the variables \"sum\" and \"term\" 10 times.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhqP7g40o0z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7198fa14-4a15-45be-aafc-6a7c4d5fdb9b"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "############# Write here. ################\n",
        "t = tf.Variable(2, name=\"term\", dtype=tf.float64)\n",
        "s = tf.Variable(0, name=\"sum\", dtype=tf.float64)\n",
        "\n",
        "for _ in range(10):\n",
        "    print('t: ', t.read_value().numpy())\n",
        "    s.assign(s+1/t)\n",
        "    t.assign(t*(t-1)+1)\n",
        "\n",
        "##########################################\n",
        "\n",
        "#print('s:', s.read_value().numpy())\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t:  2.0\n",
            "t:  3.0\n",
            "t:  7.0\n",
            "t:  43.0\n",
            "t:  1807.0\n",
            "t:  3263443.0\n",
            "t:  10650056950807.0\n",
            "t:  1.1342371305542185e+26\n",
            "t:  1.2864938683278672e+52\n",
            "t:  1.6550664732451996e+104\n",
            "<tf.Variable 'sum:0' shape=() dtype=float64, numpy=0.9999999999999999>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbiAmxry-n75"
      },
      "source": [
        "# **2. Dataset API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mHYbZmS06zb"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The `tf.data` API is the most advanced API for writing TensorFlow input pipelines.\n",
        "\n",
        "It allows you to build complex pipelines by composing simple building blocks. \n",
        "\n",
        "`tf.data.Dataset` is an abstraction representing a sequence of elements (each element represents one or more `tf.Tensor`s)\n",
        "\n",
        "Users can create new Datasets from existing `tf.Tensor`s by using static methods like `Dataset.from_tensor_slices()`. \n",
        "\n",
        "For example, you can create a Dataset of string Tensors that represents input file names. \n",
        "\n",
        "Transformation of exisiting Datasets is another way of creating new dataset. \n",
        "\n",
        "TensorFlow provides frequently-used Dataset transformations such as `Dataset.batch` or `Dataset.shuffle` (please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2N-qVWB4VNW"
      },
      "source": [
        "### `tf.data.Dataset.from_tensor_slices()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knXKv93E4drK"
      },
      "source": [
        "Creates a Dataset whose elements are slices of the given *python array* or *numpy array* or *tensors*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lVbbwCfzGbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfcc28a-e9aa-45eb-c8cf-cad85e6104df"
      },
      "source": [
        "import numpy as np\n",
        "arr = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# Create a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)#리스트를 데이터 셋으로 만들기\n",
        "\n",
        "# Iterate through the dataset\n",
        "for element in dataset:\n",
        "  # Print multiplied value for each element in the dataset\n",
        "  print((element * 2).numpy())\n",
        "\n",
        "# You can also use an iterator like this:\n",
        "# iterator = iter(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV-LVSuslOLE"
      },
      "source": [
        "## Create a dataset from files using the Dataset API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCt7x4ByogNm"
      },
      "source": [
        "### Create dummy binary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTFhI2kBomkO"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_bin_file(file_name, value):\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(np.arange(value, value+4, dtype=np.int32))\n",
        "    \n",
        "bin_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'binary_file_%d'% i#폴더를 열어서 확인을 해 보시오\n",
        "  create_bin_file(file_name, i)\n",
        "  bin_filenames.append(file_name)\n",
        "\n",
        "# first file:\n",
        "# 0 1 2 3\n",
        "\n",
        "# second file:\n",
        "# 1 2 3 4\n",
        "\n",
        "# third file:\n",
        "# 2 3 4 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rBvsoONqBSl"
      },
      "source": [
        "### FixedLengthRecordDataset : each fixed-length slice of bytes is a dataset element.\n",
        "\n",
        "In this example, each data instance is a 8-byte integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48JvZOGtqEwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8356f1aa-488d-4692-d925-608acce15719"
      },
      "source": [
        "# create a Dataset that contains slices (size: 8 bytes) of the files\n",
        "dataset = tf.data.FixedLengthRecordDataset(bin_filenames, 8)#bin_files가 세가지 파일이름이 list로 되어있다\n",
        "# or equivalently,\n",
        "# ds = tf.data.Dataset.from_tensor_slices(bin_filenames)\n",
        "# ds = ds.apply(lambda filename: tf.data.FixedLengthRecordDataset(filename, 8))\n",
        "\n",
        "# Iterate through the dataset\n",
        "for i, element in enumerate(dataset):\n",
        "  # convert 8 bytes into int32 => two int32 value per each element\n",
        "  print('step %d, data: %s' % (i, tf.io.decode_raw(element, 'int32').numpy()))#32비트 형태로 저장되어있는걸 그대로 가져와달라고 decode_raw를 사용함."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: [0 1]\n",
            "step 1, data: [2 3]\n",
            "step 2, data: [1 2]\n",
            "step 3, data: [3 4]\n",
            "step 4, data: [2 3]\n",
            "step 5, data: [4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5FwnpyyneZQ"
      },
      "source": [
        "### Create dummy text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdSZO50jlSbB"
      },
      "source": [
        "def create_text_file(file_name, index):\n",
        "  with open(file_name, 'w') as f:\n",
        "    f.write('Hello_%d\\n' % index)\n",
        "    f.write('TensorFlow_%d\\n' % index)\n",
        "\n",
        "text_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'text_file_%d'% i\n",
        "  create_text_file(file_name, i)\n",
        "  text_filenames.append(file_name)#파일이름까지 생성함.\n",
        "\n",
        "# first file:\n",
        "# Hello_0\n",
        "# TensorFlow_0\n",
        "\n",
        "# second file:\n",
        "# Hello_1\n",
        "# TensorFlow_1\n",
        "\n",
        "# third file:\n",
        "# Hello_2\n",
        "# TensorFlow_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv2Vc8gt9WbO"
      },
      "source": [
        "### TextLineDataset : each text line is a dataset element.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw5gAwbcCNPm"
      },
      "source": [
        "def iterate_and_print(iterator, count=6):\n",
        "  for i in range(count):\n",
        "    print('step %d, data: %s' % (i, next(iterator).numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYY0TdipmvoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd7964a-ba1f-41b6-c483-4794368163fd"
      },
      "source": [
        "# create a Dataset that contains each line of the text files\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "# or equivalently,\n",
        "# ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "# ds = ds.apply(lambda filename: tf.data.TextLineDataset(filename))\n",
        "\n",
        "# Create iterator for the dataset\n",
        "iterator = iter(ds)\n",
        "\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8eqeLjir-ar"
      },
      "source": [
        "## Transform dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GinK40Kp5jQA"
      },
      "source": [
        "**ds.shuffle(buffer_size)**\n",
        "\n",
        "shuffle: shuffle data instances randomly. buffer size represents the number of data instances to be sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzImNht48XKW"
      },
      "source": [
        "`ds.shuffle` with N > 1 can pick data instances randomly from the buffer containing N instances. The code snippet below shows that we always do not get the 5th or 6th element of the dataset (Hello_2 or TensorFlow_2) at step 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3oaSOHVsGLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0972d32a-c864-427d-fe24-7d26be743a00"
      },
      "source": [
        "# Load the text file created previously\n",
        "ds = tf.data.TextLineDataset(text_filenames) \n",
        "# shuffle the dataset using buffer size 4\n",
        "ds = ds.shuffle(4)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_1'\n",
            "step 1, data: b'Hello_2'\n",
            "step 2, data: b'TensorFlow_0'\n",
            "step 3, data: b'Hello_0'\n",
            "step 4, data: b'TensorFlow_1'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-MH37I8s8z"
      },
      "source": [
        "`ds.shuffle` with N == 1 has no shuffling effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Q9W2pG6nxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d66213-4b20-4319-87e3-2d7925cbd216"
      },
      "source": [
        "# Load the text file created previously\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "# shuffle the dataset using buffer size 1\n",
        "ds = ds.shuffle(1)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)#packing 하나로 했으니까 바뀌는 것이 없다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i-rJwnE6Vdj"
      },
      "source": [
        "**ds.repeat(count)**\n",
        "\n",
        "Repeat the data instances count times. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM9xpfjr72Qo"
      },
      "source": [
        "An error is raised when an iterator calls a next element after reading all the data from the dataset. `ds.repeat(count)` repeats the dataset `ds` so each original value is seen `count` times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUzyWsUt6WQ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "2dc22ca1-aaa7-4fb4-8712-684b06c53a8a"
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator, count=7) # error; 6개의 데이터를 뛰어넘는 count를 7로주면 그 다음에 데이터가 없어서 오류"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    771\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7214\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7215\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} End of sequence [Op:IteratorGetNext]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1ce33fea6bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0miterate_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# error; 6개의 데이터를 뛰어넘는 count를 7로주면 그 다음에 데이터가 없어서 오류\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-33ae1a1528ca>\u001b[0m in \u001b[0;36miterate_and_print\u001b[0;34m(iterator, count)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterate_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step %d, data: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CwPo5I76G9"
      },
      "source": [
        "`ds.repeat(count)` repeats iterating the dataset `count` times. If we do not pass the `count` argument, the dataset repeats forever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzqJY23X8Hg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80dfaa49-3ab1-4ab2-a637-fac5941fd638"
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "# repeat twice\n",
        "ds = ds.repeat(2)#데이터를 한 번 더 해서 12개가 되도록 해준다\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator, count=12) # error #error가 안나려면 count를 12로 하면 됨."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n",
            "step 6, data: b'Hello_0'\n",
            "step 7, data: b'TensorFlow_0'\n",
            "step 8, data: b'Hello_1'\n",
            "step 9, data: b'TensorFlow_1'\n",
            "step 10, data: b'Hello_2'\n",
            "step 11, data: b'TensorFlow_2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yM9UYyhdamR"
      },
      "source": [
        "Another common pattern is to use try-except clause to detect the end of epoch. Once we finisn an epoch, we re-initialize the iterator to start from the beginning again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyHGXUxadamS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf41bb4-d439-426d-b876-6df8a35ace63"
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "iterator = iter(ds)\n",
        "\n",
        "epoch = 0\n",
        "step = 0\n",
        "\n",
        "while True:\n",
        "  # repeat until we detect an error\n",
        "  try:\n",
        "    v = next(iterator).numpy()\n",
        "    print('step %d, data: %s' % (step, v))\n",
        "    step += 1\n",
        "  # iterator raises StopIteration once we finish an epoch\n",
        "  except StopIteration:\n",
        "    print('Finished epoch', epoch)\n",
        "    epoch += 1\n",
        "    # if we are done with 2 epochs, break\n",
        "    if epoch >= 2:\n",
        "      break\n",
        "    # otherwise, re-create an iterator\n",
        "    iterator = iter(ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n",
            "Finished epoch 0\n",
            "step 6, data: b'Hello_0'\n",
            "step 7, data: b'TensorFlow_0'\n",
            "step 8, data: b'Hello_1'\n",
            "step 9, data: b'TensorFlow_1'\n",
            "step 10, data: b'Hello_2'\n",
            "step 11, data: b'TensorFlow_2'\n",
            "Finished epoch 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71CqxItI4_Rk"
      },
      "source": [
        "**ds.batch(batch_size)**\n",
        "\n",
        "Combines elements of this dataset into batches. `batch_size` represents the number of data instances to combine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8stEmpyxvT18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579708a9-af50-4b85-c7ad-c508de3d9bb1"
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames) \n",
        "# batch elements using batch_size 3\n",
        "ds = ds.batch(3)#2개의 element가 들어가서 batch가 3번이 되고\n",
        "ds = ds.repeat(3)#6개의 batch\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 1, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
            "step 2, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 3, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
            "step 4, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 5, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meX0yDAd4KCD"
      },
      "source": [
        "**ds.map(fn)**\n",
        "\n",
        "Apply `fn` to each element of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zITLEwgys7l8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1834fd-a557-4f65-d2c2-cc7f474291f4"
      },
      "source": [
        "# split the `data` tensor into 3 pieces and concatenate the pieces by inserting '+' between them\n",
        "def split_join(data):\n",
        "  data = tf.split(data, 3)\n",
        "  return tf.strings.join(data, '+')\n",
        "\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.batch(3)\n",
        "ds = ds.repeat(3)\n",
        "ds = ds.map(split_join)#map이라는 함수를 넣어서 loop를 돌지 않아도 됨.\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 1, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
            "step 2, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 3, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
            "step 4, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 5, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXqUUZeo5a9E"
      },
      "source": [
        "##  Speed up Dataset processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WplDJMqQ59im"
      },
      "source": [
        "\n",
        "**ds.interleave(map_func, cycle_length)**\n",
        "\n",
        "map_func : map function to apply to each data instance\n",
        "\n",
        "cycle_length : the number of data instances to process concurrently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elNSpQlF9Kkd"
      },
      "source": [
        "We can use this feature to read and process multiple files concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlIOImd6KHHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824497aa-318b-48c1-e7ed-dc0f6af0b947"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "# consume the first two files in concurrently, and then the third file \n",
        "ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                    cycle_length=2)#동시에 여러 사람이 작업을 하면 속도 차이가 나긴 하는데 다른 것이 하는 속도대로 하고 하면 순서는 보장이 되지 않은채로 나오게된다.\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'Hello_1'\n",
            "step 2, data: b'TensorFlow_0'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhp0R08AKxpy"
      },
      "source": [
        "**ds.prefetch(buffer_size)** \n",
        "\n",
        "prefetch elements from a dataset. buffer size represents the maximum buffer size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWM2PH6SLHrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303be9fe-6cf3-48df-a45b-33c4d1bbbb5c"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                    cycle_length=2)\n",
        "ds = ds.prefetch(3)#dataset의 경우 \n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'Hello_1'\n",
            "step 2, data: b'TensorFlow_0'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEs3qEJszYgu"
      },
      "source": [
        "## Quiz 3\n",
        "Create a dataset following the instructions. (data type should be number, not string)\n",
        "\n",
        "1. Create a textline dataset using files named `ex_filenames`. \n",
        "2. Shuffle the dataset with buffer size 15.\n",
        "3. Repeat the dataset for 2 epochs.\n",
        "4. Convert each data instance using the `cast` function defined below.\n",
        "5. Make the data instances as a batch (batch size = 3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_TOoWhx1O2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51dbc22-50dd-4a01-8683-abaaa8f5a797"
      },
      "source": [
        "import random\n",
        "\n",
        "def create_text_file(index):\n",
        "  with open('ex_file_%d'%index, 'w') as f:\n",
        "    for i in range(3):\n",
        "      f.write('%d.%d\\n' % (index, i))\n",
        "    \n",
        "ex_filenames = []\n",
        "for i in range(5):\n",
        "  create_text_file(i)\n",
        "  ex_filenames.append('ex_file_%d'% i)\n",
        "\n",
        "def cast(data):\n",
        "  data = tf.strings.to_number(data, out_type=tf.float32)\n",
        "  return data\n",
        "\n",
        "\n",
        "############# Write here. ################\n",
        "# Create a Dataset\n",
        "ds = tf.data.TextLineDataset(ex_filenames)\n",
        "# Shuffle\n",
        "ds = ds.shuffle(15)\n",
        "# Repeat\n",
        "ds = ds.repeat(2)\n",
        "# Transformation\n",
        "ds = ds.map(cast)\n",
        "# Create a mini-batch\n",
        "ds = ds.batch(3)\n",
        "\n",
        "##########################################\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator, count=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, data: [3.2 2.1 0.2]\n",
            "step 1, data: [3.1 3.  4.1]\n",
            "step 2, data: [1.  4.2 4. ]\n",
            "step 3, data: [1.2 0.1 2.2]\n",
            "step 4, data: [1.1 2.  0. ]\n",
            "step 5, data: [3.  3.2 3.1]\n",
            "step 6, data: [4.1 4.2 1.1]\n",
            "step 7, data: [0.1 4.  2.1]\n",
            "step 8, data: [1.  2.  2.2]\n",
            "step 9, data: [1.2 0.  0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO0i-ksu8jB9"
      },
      "source": [
        "# **FYI: TensorFlow v1 vs. TensorFlow v2** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDH6BhQ3P7ou"
      },
      "source": [
        "Throughout the tutorial, we used the latest release of TensorFlow. Check out the version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn3qNA6uP7BY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b52082dd-5f51-421e-8c03-7db9ae5aa32e"
      },
      "source": [
        "print(\"TensorFlow version: \", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version:  2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thHFaK0L8nqp"
      },
      "source": [
        "Previously in TensorFlow v1, we construct a graph using **Graph** which consists of TensorFlow operations (Ops). \n",
        "\n",
        "After defining a Graph, we could run it via **Session**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22S2dhxtqV1m"
      },
      "source": [
        "# graph = tf.Graph()\n",
        "# with graph.as_default():\n",
        "      # Dataset\n",
        "      # Build a model\n",
        "      # Training\n",
        "#     // Here we load dataset, define operations, etc.\n",
        "\n",
        "# with tf.Session(graph=graph) as sess:\n",
        "#     sess.run(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXypoQu4Qn53"
      },
      "source": [
        "Switching from TensorFlow v1 to TensorFlow v2, there have been many things changed.\n",
        "To sum up the update, TensorFlow shifted to **eager execution** (imperative execution) by default. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipQBjqopUOWM"
      },
      "source": [
        "Eager execution provides an intuitive interface to structure the code naturally and use Python data structures. It is also easier to debug and test changes by using standard Python debugging tools.\n",
        "Lastly, it has natural Python control flow instead of graph control flow, simplifying the specification of dynamic models. The downside is that eager execution is much slower than symbolic execution, the default TensorFlow v1 mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXLES0lRVuVJ"
      },
      "source": [
        "Let's see the difference of the two versions with an example of dynamic control flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwwvlb_7V4Fo"
      },
      "source": [
        "## Dynamic control flow\n",
        "A major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write fizzbuzz game where any number divisible by three is replaced with the word \"fizz\", and any number divisible by five is replaced with the word \"buzz\" (similar to the 3-6-9 game)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhfCip_FV1j3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b384867-7b86-4f90-bf0a-61121fd1bb55"
      },
      "source": [
        "# Native python code\n",
        "def fizzbuzz(max_num):\n",
        "    counter = 0\n",
        "    for num in range(1, max_num+1):\n",
        "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
        "            print('FizzBuzz')\n",
        "        elif int(num % 3) == 0:\n",
        "            print('Fizz')\n",
        "        elif int(num % 5) == 0:\n",
        "            print('Buzz')\n",
        "        else:\n",
        "            print(num)\n",
        "        counter += 1\n",
        "    \n",
        "fizzbuzz(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "Fizz\n",
            "4\n",
            "Buzz\n",
            "Fizz\n",
            "7\n",
            "8\n",
            "Fizz\n",
            "Buzz\n",
            "11\n",
            "Fizz\n",
            "13\n",
            "14\n",
            "FizzBuzz\n",
            "16\n",
            "17\n",
            "Fizz\n",
            "19\n",
            "Buzz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgAzIPm5WJ8R"
      },
      "source": [
        "In eager execution, we need to add minor changes \n",
        "in a few lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIb2eV37WOPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2b3291-1d7a-4d68-93f0-1070c87b5773"
      },
      "source": [
        "def fizzbuzz_eager(max_num):\n",
        "    counter = tf.constant(0) # counter = 0\n",
        "    max_num = tf.convert_to_tensor(max_num) #\n",
        "    for num in range(1, max_num.numpy() + 1): #\n",
        "        num = tf.constant(num) # \n",
        "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
        "            print('FizzBuzz')\n",
        "        elif int(num % 3) == 0:\n",
        "            print('Fizz')\n",
        "        elif int(num % 5) == 0:\n",
        "            print('Buzz')\n",
        "        else:\n",
        "            print(num.numpy()) # print(num)\n",
        "        counter += 1\n",
        "    \n",
        "fizzbuzz_eager(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "Fizz\n",
            "4\n",
            "Buzz\n",
            "Fizz\n",
            "7\n",
            "8\n",
            "Fizz\n",
            "Buzz\n",
            "11\n",
            "Fizz\n",
            "13\n",
            "14\n",
            "FizzBuzz\n",
            "16\n",
            "17\n",
            "Fizz\n",
            "19\n",
            "Buzz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QGYXcyyWRzN"
      },
      "source": [
        "Implementing the same thing using graph mode in TensorFlow v1: [Tensorflow FizzBuzz Revisited (Ricky Han blog)](https://rickyhan.com/jekyll/update/2018/02/16/tensorflow-fizzbuzz-revisited.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QpFmaoDWaRU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "2173ed55-d095-4a42-fcd8-627dc0a77a71"
      },
      "source": [
        "# This code does not run in tensorflow 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def fizzbuzz_graph(max_num):\n",
        "    # Define variable and while_loop\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        arr = tf.Variable([str(i) for i in range(1, max_num+1)])\n",
        "        # nasty tf.while_loop and tf.cond ops\n",
        "        while_op = tf.while_loop(\n",
        "            (lambda i, _: tf.less(i, max_num+1)), \n",
        "            (lambda i, _: (tf.add(i,1), tf.cond(\n",
        "                tf.logical_and(tf.equal(tf.mod(i, 3), 0), tf.equal(tf.mod(i, 5), 0)),\n",
        "                (lambda : tf.assign(arr[(i - 1)], 'FizzBuzz')),\n",
        "                (lambda : tf.cond(tf.equal(tf.mod(i, 3), 0),\n",
        "                    (lambda : tf.assign(arr[(i - 1)], 'Fizz')),\n",
        "                    (lambda : tf.cond(tf.equal(tf.mod(i, 5), 0),\n",
        "                        (lambda : tf.assign(arr[(i - 1)], 'Buzz')),\n",
        "                        (lambda : arr)))))))),\n",
        "            [1, arr])\n",
        "\n",
        "    # Call Session.run()\n",
        "    with tf.Session(graph = graph) as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        idx, array = sess.run(while_op)\n",
        "        print(array)\n",
        "\n",
        "\n",
        "fizzbuzz_graph(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bddc2b2b8f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mfizzbuzz_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-bddc2b2b8f2f>\u001b[0m in \u001b[0;36mfizzbuzz_graph\u001b[0;34m(max_num)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# nasty tf.while_loop and tf.cond ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         while_op = tf.while_loop(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             (lambda i, _: (tf.add(i,1), tf.cond(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2515\u001b[0m   \"\"\"\n\u001b[0;32m-> 2516\u001b[0;31m   return while_loop(\n\u001b[0m\u001b[1;32m   2517\u001b[0m       \u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m       \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2714\u001b[0m   if (util.EnableControlFlowV2(ops.get_default_graph()) and\n\u001b[1;32m   2715\u001b[0m       not executing_eagerly):\n\u001b[0;32m-> 2716\u001b[0;31m     return while_v2.while_loop(\n\u001b[0m\u001b[1;32m   2717\u001b[0m         \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, maximum_iterations, name, return_same_structure, back_prop)\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloop_counter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_iterations_arg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     body_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mbody_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mwrapped_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36mwrapped_body\u001b[0;34m(loop_counter, maximum_iterations_arg, *args)\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;31m# converts flows in `args` to TensorArrays and packs it into the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;31m# structure of `loop_vars_signature`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m       outputs = body(\n\u001b[0m\u001b[1;32m    201\u001b[0m           *_pack_sequence_as(loop_vars_signature, flat_orig_loop_vars, args))\n\u001b[1;32m    202\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-bddc2b2b8f2f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, _)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             (lambda i, _: (tf.add(i,1), tf.cond(\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FizzBuzz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 (lambda : tf.cond(tf.equal(tf.mod(i, 3), 0),\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'mod'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10w0TYJN8p5L"
      },
      "source": [
        "TensorFlow2 can still benefit from graph-based execution as it provides `tf.function` where we can define a function with operations. TensorFlow constructs a graph for the function automatically and apply possible optimizations.\n",
        "\n",
        "For more information on `tf.function`, refer to this website: https://www.tensorflow.org/guide/function?hl=ko"
      ]
    }
  ]
}