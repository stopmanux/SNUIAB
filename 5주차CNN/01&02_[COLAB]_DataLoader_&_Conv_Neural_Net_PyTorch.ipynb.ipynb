{"cells":[{"cell_type":"markdown","metadata":{"id":"WTi9QV3GFgu1"},"source":["# 딥러닝 모델을 학습시키기 위해 준비되어야 할 4가지 요소\n","\n","**1. 데이터**\n","2. 모델\n","3. 손실 함수(목적함수, objective function, loss function 등으로 불려요): 정답과 모델의 예측값을 어떤 식으로 비교할지 결정해주는 함수\n","4. optimizer: gradient descent를 해줄 애. 즉, 모델의 파라미터를 어느 방향으로 조금 수정할지 결정하고 수정해주는 함수"]},{"cell_type":"markdown","metadata":{"id":"a1L6kp-qFgu2"},"source":["# Part 1 : Data Loader"]},{"cell_type":"code","source":["!sudo apt-get update -y\n","!sudo apt-get install python3.7"],"metadata":{"id":"_kzwJdlRQkt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1"],"metadata":{"id":"xQGghKWcP-W5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo update-alternatives --config python3"],"metadata":{"id":"k8ozaGbAQC9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version"],"metadata":{"id":"NKN9nGF1QyKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt install python3-pip\n","!sudo apt-get install python3.7-distutils"],"metadata":{"id":"HCyKuZ4tR7me"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZjIAb54Fgu3"},"outputs":[],"source":["!pip install scipy==1.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKcRtCsbFgu3"},"outputs":[],"source":["import numpy as np\n","import os, sys\n","import tarfile\n","import matplotlib.pyplot as plt\n","\n","from IPython.display import display, Image\n","from scipy import misc\n","from six.moves.urllib.request import urlretrieve\n","from six.moves import cPickle as pickle\n","from tqdm import tqdm\n","\n","# Config the matplotlib backend as plotting inline in Ipython\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"r_UR8TmPFgu4"},"source":["## 데이터 다운로드 하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAufQ--4Fgu4"},"outputs":[],"source":["url = 'https://commondatastorage.googleapis.com/books1000/'\n","\n","data_root = './data'\n","if not os.path.exists(data_root):\n","    os.mkdir(data_root)\n","    \n","def dataset_downloader(filename):\n","    \"\"\"데이터셋 파일이 없으면 다운로드 합니다.\"\"\"\n","    dest_dir = os.path.join(data_root, filename)\n","    if not os.path.exists(dest_dir):\n","        print('다운로드 시도 중 : ', filename)\n","        filename, _ = urlretrieve(url + filename, dest_dir)\n","        print(filename, ' 다운로드 완료!')\n","    else:\n","        print(dest_dir, ' 이미 있습니다.')\n","    \n","    return dest_dir\n","\n","train_filename = dataset_downloader('notMNIST_large.tar.gz')\n","test_filename = dataset_downloader('notMNIST_small.tar.gz')"]},{"cell_type":"markdown","metadata":{"id":"aSPiB8-ZFgu5"},"source":["## 다운로드한 데이터 압축 풀기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEy2B7c1Fgu5"},"outputs":[],"source":["num_classes = 10\n","np.random.seed(1000)\n","\n","def data_extract(filename):\n","    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n","    \n","    if os.path.isdir(root):\n","        print('{} 이미 있습니다 - {} 는 추출을 건너뜁니다.'.format(root, filename))\n","    else:\n","        print('{} 에서 데이터를 추출합니다.'.format(root))\n","        tar = tarfile.open(filename)\n","        sys.stdout.flush()\n","        tar.extractall(data_root)\n","        tar.close()\n","    data_folders = [\n","        os.path.join(root, d) for d in sorted(os.listdir(root))\n","        if os.path.isdir(os.path.join(root, d))]\n","    \n","    if len(data_folders) != num_classes:\n","        raise Exception('{} folders 기대했는데, {} 개가 있네요.'.format(num_classes, len(data_folders)))\n","    \n","    print(data_folders)\n","    return data_folders\n","\n","train_folders = data_extract(train_filename)\n","test_folders = data_extract(test_filename)"]},{"cell_type":"markdown","metadata":{"id":"TP_Uh3HtFgu5"},"source":["## 각 글자 데이터를 로드해서 pickle 파일 형태로 저장하기\n","주의: 개인 노트북 컴퓨터일 경우 오래 걸릴 수 있습니다."]},{"cell_type":"code","source":["!pip install imageio\n","!pip install av\n","#import imageio\n","import imageio.v3 as iio"],"metadata":{"id":"QtH0p3m6TfHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKkTBdzAFgu6"},"outputs":[],"source":["image_size = 28  # Pixel width and height.\n","pixel_depth = 255.0  # Number of levels per pixel.\n","\n","def load_letter(folder):\n","    \"\"\"한 글자 클래스 데이터를 로드합니다.\"\"\"\n","    image_files = os.listdir(folder)\n","    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n","    \n","    num_images = 0\n","    for image in image_files:\n","        image_file = os.path.join(folder, image)\n","        try:\n","            image_data = (iio.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth  # image 픽셀값의 범위를 -0.5 ~ 0.5로 만들어줍니다.\n","            if image_data.shape != (image_size, image_size):\n","                raise Exception('이미지가 이상한 크기인데요?: {}'.format(str(image_data.shape)))\n","            dataset[num_images, :, :] = image_data\n","            num_images = num_images + 1\n","        except IOError as e:\n","            print('{} - skip'.format(e))\n","    \n","    dataset = dataset[0:num_images, :, :]\n","    print('전체 데이터셋 모양은 다음과 같습니다:', dataset.shape)\n","    \n","    return dataset\n","        \n","def make_pickle(data_folders):\n","    dataset_names = []\n","    for folder in data_folders:\n","        set_filename = folder + '.pickle'\n","        dataset_names.append(set_filename)\n","        if os.path.exists(set_filename):\n","            print('{} 이미 있습니다 - pickling을 건너뜁니다.'.format(set_filename))\n","            continue\n","        print('Pickling {}'.format(set_filename))\n","        dataset = load_letter(folder)\n","        with open(set_filename, 'wb') as f:\n","            pickle.dump(dataset, f)\n","\n","    return dataset_names\n","\n","train_datasets = make_pickle(train_folders)\n","test_datasets = make_pickle(test_folders)"]},{"cell_type":"markdown","metadata":{"id":"2lq3cugQFgu6"},"source":["## 이미지 예시 보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVkqQ3W-Fgu7"},"outputs":[],"source":["images = []\n","for i in tqdm(range(len(train_datasets))):\n","    set_filename = train_datasets[i]\n","    with open(set_filename, 'rb') as f:\n","        dataset = pickle.load(f)\n","    images.append(dataset[1])\n","print(np.shape(images))\n","\n","Row = 2\n","Column = 5\n","for i, image in enumerate(images):\n","    plt.subplot(Row, Column, i+1)\n","    plt.title('Label = {}'.format(os.path.basename(train_datasets[i])[0]))\n","    plt.imshow(image, cmap='gray')\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gwVu6wK0Fgu7"},"source":["## 트레이닝셋, 테스트셋 개수 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_lHINUJFgu7"},"outputs":[],"source":["print(\"==== 트레이닝셋을 위한 데이터들 ====\")\n","for i in range(len(train_datasets)):\n","    set_filename = train_datasets[i]\n","    with open(set_filename, 'rb') as f:\n","        dataset = pickle.load(f)\n","    print(\"글자 {} 에 대한 트레이닝 데이터 개수는 {} 개입니다.\".format(os.path.basename(set_filename)[0], len(dataset)))\n","\n","print(\"\\n==== 테스트셋을 위한 데이터들 =====\")\n","for i in range(len(test_datasets)):\n","    set_filename = test_datasets[i]\n","    with open(set_filename, 'rb') as f:\n","        dataset = pickle.load(f)\n","    print(\"글자 {} 에 대한 테스트 데이터 개수는 {} 개입니다.\".format(os.path.basename(set_filename)[0], len(dataset)))"]},{"cell_type":"markdown","metadata":{"id":"GmDAf5G5Fgu7"},"source":["## 각 글자 별로 나뉘어져있던 데이터를 합쳐서 트레이닝셋, 테스트셋 2개로 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOO-kwe_Fgu7"},"outputs":[],"source":["def merge_datasets(pickle_files, dataset_size):\n","    num_classes = len(pickle_files)\n","    dataset = np.ndarray((dataset_size, image_size, image_size), dtype=np.float32)\n","    labels = np.ndarray(dataset_size, dtype=np.int32)\n","    tsize_per_class = dataset_size // num_classes\n","\n","    start_t = 0\n","    end_t = tsize_per_class\n","    for label, pickle_file in enumerate(pickle_files):       \n","        with open(pickle_file, 'rb') as f:\n","            letter_set = pickle.load(f)\n","            np.random.shuffle(letter_set)\n","\n","            letter = letter_set[0:tsize_per_class, :, :]\n","            dataset[start_t:end_t, :, :] = letter\n","            labels[start_t:end_t] = label\n","            start_t += tsize_per_class\n","            end_t += tsize_per_class\n","\n","    return dataset, labels\n","\n","train_size = 200000\n","test_size = 10000\n","\n","train_dataset, train_labels = merge_datasets(train_datasets, train_size)\n","test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n","\n","print('Training:', train_dataset.shape, train_labels.shape)\n","print('Testing:', test_dataset.shape, test_labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"zsfTbVWaFgu8"},"source":["### 데이터 섞어주기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsp5lqMvFgu8"},"outputs":[],"source":["def shuffle(dataset, labels):\n","    permutation = np.random.permutation(labels.shape[0])\n","    shuffled_dataset = dataset[permutation,:,:]\n","    shuffled_labels = labels[permutation]\n","    return shuffled_dataset, shuffled_labels\n","\n","train_dataset, train_labels = shuffle(train_dataset, train_labels)\n","test_dataset, test_labels = shuffle(test_dataset, test_labels)"]},{"cell_type":"markdown","metadata":{"id":"sFeHoCHVFgu8"},"source":["### 섞은 후에 다시 한 번 보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFtfJ2SIFgu8"},"outputs":[],"source":["Row = 2\n","Column = 5\n","ListOfLabel = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n","\n","print(\"==== 트레이닝 데이터 예시 ====\")\n","images = train_dataset[0:10]\n","labels = train_labels[0:10]\n","for i, image in enumerate(images):\n","    plt.subplot(Row, Column, i+1)\n","    plt.title('Label = {}'.format(ListOfLabel[labels[i]]))\n","    plt.imshow(image, cmap='gray')\n","    plt.axis('off')\n","plt.show()\n","\n","print(\"==== 테스트 데이터 예시 ====\")\n","images = test_dataset[0:10]\n","labels = test_labels[0:10]\n","for i, image in enumerate(images):\n","    plt.subplot(Row, Column, i+1)\n","    plt.title('Label = {}'.format(ListOfLabel[labels[i]]))\n","    plt.imshow(image, cmap='gray')\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dwZNaPgpFgu8"},"source":["## 파일로 내보내기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Nu6udWFFgu9"},"outputs":[],"source":["pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n","\n","f = open(pickle_file, 'wb')\n","save = {\n","    'train_dataset': train_dataset[:50000,],\n","    'train_labels': train_labels[:50000,],\n","    'test_dataset': test_dataset[:5000,],\n","    'test_labels': test_labels[:5000,],\n","}\n","pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"uCBN5rcHFgu9"},"source":["# 딥러닝 모델을 학습시키기 위해 준비되어야 할 4가지 요소\n","\n","**1. 데이터**\n","\n","2. 모델\n","3. 손실 함수(목적함수, objective function, loss function 등으로 불려요): 정답과 모델의 예측값을 어떤 식으로 비교할지 결정해주는 함수\n","4. Optimizer: gradient descent를 해줄 애. 즉, 모델의 파라미터를 어느 방향으로 조금 수정할지 결정하고 수정해주는 함수"]},{"cell_type":"markdown","metadata":{"id":"wFRw-uwxFgu9"},"source":["# Part 2: PyTorch를 이용해서 Convolutional Neural Network 정의해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dN0_YIPtFgu9"},"outputs":[],"source":["%matplotlib inline\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import random\n","import numpy as np\n","from six.moves import range\n","from six.moves import cPickle as pickle\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"_q640rnnFgu9"},"source":["## 모델에 데이터 넣을 준비하기\n","\n","우선 사용하고 싶은 데이터 파일이 있다면 \n","1. 그걸 우선 numpy array 형식으로 불러와야 해요.\n","2. 그리고는 필요한 전처리를 해준 후에 \n","3. 이 numpy array를 `torch.*Tensor` 형식으로 변환하고 \n","4. dataloader에 넣어주면 pytorch로 짠 딥러닝 모델에 넣을 준비가 된 거예요.\n","\n","대개 이제 이런 데이터 처리를 도와주는 패키지들이 있는데<br/>\n","이미지는 openCV, Pillow를 많이 쓰고,<br/>\n","텍스트에는 SpaCy를 많이 사용합니다. <br/>\n","\n","여기서는 매우 간단한 이미지 데이터를 사용하므로 별도의 패키지를 쓰지는 않을게요 "]},{"cell_type":"markdown","metadata":{"id":"oVt6nii6Fgu9"},"source":["### 1. 데이터셋 불러오기\n","\n","이미 앞서서 numpy형식으로 저장을 해뒀죠? <br/>\n","그러니까 그냥 로드만 해주면 됩니다. <br/>\n","고마워 pickle! <br/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txeL6_MJFgu9"},"outputs":[],"source":["pickle_file = 'data/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","    save = pickle.load(f)\n","    train_dataset = save['train_dataset']\n","    train_labels = save['train_labels']\n","    test_dataset = save['test_dataset']\n","    test_labels = save['test_labels']\n","    del save  # garbage collector(gc)에게 \"이거 지워도 된단다\"라고 넌지시 알려주기 \n","    print('Training set', train_dataset.shape, train_labels.shape)\n","    print('Test set', test_dataset.shape, test_labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"_y1xMmAtFgu-"},"source":["### ~심심하니까~ 이미지 한 번 더 확인해줍니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bE_pi_CYFgu-"},"outputs":[],"source":["Row = 2\n","Column = 5\n","classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n","\n","print(\"==== 트레이닝 데이터 예시 ====\")\n","rand = random.randint(0, 29989)  # 그냥 아무 랜덤한 이미지들을 뽑아내기 위해서 index 역할을 해줄 숫자 하나를 뽑아볼게요.\n","images = train_dataset[rand:rand+10]\n","labels = train_labels[rand:rand+10]\n","print(labels)\n","for i, image in enumerate(images):\n","    plt.subplot(Row, Column, i+1)\n","    plt.title('Label = {}'.format(classes[labels[i]]))\n","    plt.imshow(image, cmap='gray')\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ap-6fFn4Fgu-"},"source":["### 2. 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tk-8yBXdFgu-"},"outputs":[],"source":["image_size = 28  # notMNIST 데이터는 이미지 크기가 28x28이였죠?\n","num_labels = 10  # 클래스 개수는 10개구요.\n","\n","# 트레이닝셋, 테스트셋 이미지들의 차원 배치를 좀 바꿔줍니다.\n","x_train = train_dataset.reshape(-1, 1, 28, 28)  # 왜 1일까요?\n","x_test = test_dataset.reshape(-1, 1, 28, 28)\n","\n","y_train = train_labels\n","y_test = test_labels\n","\n","print(x_train.shape, x_test.shape)\n","print(y_train.shape, y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"MnDMrfY0Fgu-"},"source":["### 3. tensor로 바꿔주기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_T9p5pWPFgu-"},"outputs":[],"source":["x_train = torch.tensor(x_train)\n","y_train = torch.tensor(y_train, dtype=torch.int64)\n","\n","x_test = torch.tensor(x_test)\n","y_test = torch.tensor(y_test, dtype=torch.int64)"]},{"cell_type":"markdown","metadata":{"id":"plPPPl5xFgu-"},"source":["### 4. data loader에 넣어주기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwNxAj1HFgu-"},"outputs":[],"source":["train_set = torch.utils.data.TensorDataset(x_train, y_train)\n","test_set = torch.utils.data.TensorDataset(x_test, y_test)\n","\n","batch_size = 16  # 데이터를 모델에 16개씩 넣어주겠다는 의미예요\n","train_loader = torch.utils.data.DataLoader(dataset=train_set, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_set, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"VqvpNU96Fgu_"},"source":["## 학습 이미지 예시 보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVMVfOn6Fgu_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# functions to show an image\n","def imshow(img):\n","    img = img + 0.5     # 아까 앞에서 normalize해줘서 색깔 이미지가 이상해져있을 거기 때문에 보기 편하라고 다시 unnormalize해줍니다\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# get some random training images\n","dataiter = iter(train_loader)\n","#images, labels = dataiter.next()\n","images, labels = next(dataiter)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images[:8]))\n","# print labels\n","print(' '.join('%5s' % classes[labels[j]] for j in range(8)))"]},{"cell_type":"markdown","metadata":{"id":"__JnBD-jFgu_"},"source":["# 딥러닝 모델을 학습시키기 위해 준비되어야 할 4가지 요소\n","\n","1. 데이터\n","\n","**2. 모델**\n","3. Loss function (손실함수, 목적함수, objective function 등으로 불려요): 정답과 모델의 예측값을 어떤 식으로 비교할지 결정해주는 함수\n","4. Optimizer: gradient descent를 해줄 애. 즉, 모델의 파라미터를 어느 방향으로 조금 수정할지 결정하고 수정해주는 함수"]},{"cell_type":"markdown","metadata":{"id":"Ruorx2yyFgvA"},"source":["## 모델 정의하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lw4n81qTFgvA"},"outputs":[],"source":["import torch.nn as nn\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        self.conv1 = nn.Conv2d(1, 6, 3, stride=1, padding=1)  # yes, zero padding. \n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.relu = nn.ReLU()\n","        ############### TODO: fc layer를 완성 해보세요 ################\n","        \n","        # self.fc = nn.Linear(인풋 숫자 , 아웃풋 숫자 )\n","        \n","        ########################################################\n","\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","        # print(\"view 이전 x의 모양: \", x.shape)\n","        \n","        # 위 3줄을 줄여서 표현한 게 아래예요.\n","        # x = self.pool(self.relu(self.conv1(x)))\n","\n","        ############### TODO: 빈칸을 완성 해보세요 ################\n","        \n","        # x = x.view(-1, 여긴 뭐가 들어가야 할까요)  # 얘의 기능은 텐서의 모양을 원하는 모양으로 바꿔주는 거예요\n","        # 예시: x가 만약 (16, 3, 12, 12) 모양이었다면 x.view(-1, 144)는 (48, 144) 모양으로 바꿔준답니다. -1은 나머지 숫자를 자동으로 채워주는 역할이에요\n","        # print(\"view 이후 x의 모양: \", x.shape)\n","        \n","        ########################################################\n","        x = self.fc(x)\n","\n","        return x\n","    \n","\n","net = Net()"]},{"cell_type":"markdown","metadata":{"id":"s1f_kIQwFgvB"},"source":["# 딥러닝 모델을 학습시키기 위해 준비되어야 할 4가지 요소\n","\n","1. 데이터\n","2. 모델\n","\n","**3. Loss function (손실함수, 목적함수, objective function 등으로 불려요)**: 정답과 모델의 예측값을 어떤 식으로 비교할지 결정해주는 함수\n","\n","**4. Optimizer**: gradient descent를 해줄 애. 즉, 모델의 파라미터를 어느 방향으로 조금 수정할지 결정하고 수정해주는 함수"]},{"cell_type":"markdown","metadata":{"id":"Pt3XxfWnFgvB"},"source":["## Loss function과 Optimizer 정의하기\n","\n","Cross-entropy loss function과 SGD optimizer를 씁니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCu7D7sSFgvC"},"outputs":[],"source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"M9T5p4HXFgvD"},"source":["## 모델 학습시키기\n","\n","이제\n","* data loader\n","* model\n","* loss function\n","* optimizer\n","\n","이 4가지가 모두 준비되었으니 학습을 할 준비가 끝났습니다.\n","\n","```\n","종료 조건 만족할 때까지 아래를 반복:\n","    1. 우리의 data loader로부터 데이터를 받아와서 모델에 넣어주고\n","    2. 모델의 출력 값을 받아서 \n","    3. loss function 값을 계산하고\n","    4. 그 loss를 바탕으로 backprop(=gradient를 계산) 해준 뒤 \n","    5. optimizer가 gradient descent를 1 step 진행합니다.\n","    ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdgvrsEoFgvD"},"outputs":[],"source":["loss_history = []  # 우리 모델이 어떻게 학습되는지, loss가 잘 떨어지는지 확인하기 위해 매 step loss를 저장해놓을 배열"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-J7FXcZFgvD"},"outputs":[],"source":["for epoch in range(2):  # 전체 데이터셋을 몇 번 반복할 건지\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # data에는 이미지와 정답 라벨이 들어있죠?\n","        inputs, labels = data\n","        \n","        # Batch_dim, Channel_dim, Height, Width: BCHW라고 줄여서 말하기도 합니다. 또는 NCHW\n","        # inputs의 모양은 (16, 1, 28, 28)일 거예요. batch_size 16개, channel 1개, height, width 각 28.\n","        # 흔히 이걸 batch_first 형태라고 해요.\n","        \n","        # 매 반복마다 이전 gradient를 한 번 지워줍니다.\n","        optimizer.zero_grad()\n","\n","        # 모델에 데이터 넣어서 forward 해주고 \n","        # backprop(=backward)으로 이번 input에 대해 gradient를 계산해주고\n","        # optimizer가 gradient descent 1스텝 진행\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # 결과치 화면에 뿌려주기\n","        running_loss += loss.item()\n","        if i % 300 == 299:    \n","            \n","            # 300 미니배치마다 트레이닝셋에 대한 loss값 출력\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 300))\n","            \n","            # 나중에 시각화를 위해 중간중간 따로 loss값 저장\n","            loss_history.append(running_loss / 300)\n","            running_loss = 0.0\n","            \n","print('학습 끝!')"]},{"cell_type":"markdown","metadata":{"id":"0vBLJRdBFgvE"},"source":["### 학습 경과 살펴보기"]},{"cell_type":"markdown","metadata":{"id":"JWgjxYjXFgvE"},"source":["사실 저렇게 출력된 숫자들만 보면 감이 잘 오지 않는 거 같아요 그쵸?<br/>\n","그래서 그래프로 트레이닝셋에 대해 loss가 잘 떨어지는지 살펴보는 게 필요해요<br/>\n","우리가 방금 위에서 정의한 loss_history 배열에는 학습 중간중간 저장해놓았던 트레이닝셋에 대한 loss 값들이 있어요. <br/>\n","얘를 시각화 해볼게요"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnkHuWebFgvE"},"outputs":[],"source":["plt.plot(loss_history) \n","plt.title('Training Loss', fontsize=20)  # 여기에 한글을 넣고 싶으시다구요? 그럼 좀 귀찮은 몇 가지 작업들을 해야 합니다... 그러므로 패스\n","plt.xlabel('Iteration',fontsize=16)\n","plt.ylabel('Loss',fontsize=16)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Tmea-dqMFgvF"},"source":["## 모델 저장하기\n","\n","학습이 끝난 모델의 파라미터를 저장해두면 나중에 필요할 때 불러와서 가져다 쓰면 바로 사용할 수 있어요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxlj_HH8FgvF"},"outputs":[],"source":["PATH = './notmnist_net.pth'\n","torch.save(net.state_dict(), PATH)"]},{"cell_type":"markdown","metadata":{"id":"yeSoYiiaFgvF"},"source":["## 테스트셋에 검증해보기\n","\n","이제 모델 학습이 끝났으니 테스트 데이터에도 잘하는지 확인을 해봐야 합니다. <br/>\n","테스트셋 데이터 중 몇 개나 맞히는지 알아볼까요?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMBKZWQUFgvF"},"outputs":[],"source":["# 테스트셋 이미지 예시도 심심풀이로 한 번 확인해보기\n","# functions to show an image\n","def imshow(img):\n","    img = img + 0.5     # 아까 앞에서 normalize해줘서 색깔 이미지가 이상해져있을 거기 때문에 보기 편하라고 다시 unnormalize해줍니다\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","dataiter = iter(test_loader)\n","#images, labels = dataiter.next()\n","images, labels = next(dataiter)\n","\n","imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"dXnxx4fyFgvF"},"source":["### 아까 저장해뒀던 모델 파라미터 불러오기\n","\n","사실 굳이 불러오지 않고 그냥 위에 있는 `net` 그대로 써도 되지만 <br/>\n","일단 어떻게 저장하고 불러오는지 여러분이 알아둬야 하니까 여기서는 `net`에 굳이 다시 불러와봤어요"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAUoeGRQFgvG"},"outputs":[],"source":["net = Net()\n","net.load_state_dict(torch.load(PATH))"]},{"cell_type":"markdown","metadata":{"id":"nk1rjmM3FgvG"},"source":["### 불러온 모델로 예측해보기\n","\n","이미지들을 넣었을 때 모델이 뭐라고 예측하는지 한 번 확인해볼게요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axDMUoqdFgvG"},"outputs":[],"source":["outputs = net(images)\n","outputs"]},{"cell_type":"markdown","metadata":{"id":"26iJkdgGFgvG"},"source":["뭔지 전혀 모르겠죠? <br/>\n","각 row에 있는 숫자들은 10개의 클래스에 대한 logit 값이에요. (확률 값이 아니라) <br/>\n","어떤 인덱스의 logit값이 크면 모델은 그 해당 인덱스의 클래스로 해당 이미지를 분류한다는 의미입니다. <br/>\n","따라서 그냥 이 logit 값들 중 제일 큰 logit이 있는 index를 각 row마다 뽑아오면 됩니다. <br/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAvjqBjeFgvG"},"outputs":[],"source":["_, predicted = torch.max(outputs, 1)  # 1번째 차원(=각 row)에서 각각 max인 값과 해당 index를 뽑아옵니다.\n","\n","classes = ['A', 'B', 'C', 'D', 'E', 'F','G', 'H', 'I', 'J']\n","\n","print('모델 예측: ', ' '.join('%5s' % classes[predicted[j]]\n","                              for j in range(8)))"]},{"cell_type":"markdown","metadata":{"id":"QDXmcLN6FgvG"},"source":["#### 역시 원래 이미지랑 같이 봐야 감이 올 거 같죠?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mP0mYh5mFgvH"},"outputs":[],"source":["imshow(torchvision.utils.make_grid(images[:8]))\n","print('실제 정답: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))\n","print('모델 예측: ', ' '.join('%5s' % classes[predicted[j]]\n","                              for j in range(8)))"]},{"cell_type":"markdown","metadata":{"id":"pfJLWX7JFgvH"},"source":["매우 잘 맞히는 거 같습니다! <br/>\n","그러면 이제 전체 테스트셋에 대해 정답과 비교해서 몇 개나 맞히는지 보겠습니다. "]},{"cell_type":"markdown","metadata":{"id":"UDAIpaSDFgvH"},"source":["### 테스트셋 정답률 확인해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g9kYCXGFgvH"},"outputs":[],"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('10000개의 테스트 이미지에 대한 정답률: %d %%' % (\n","    100 * correct / total))"]},{"cell_type":"markdown","metadata":{"id":"YIaxUkc8FgvH"},"source":["### 각 클래스 별 정답률 확인해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jWE_FC8FgvH"},"outputs":[],"source":["class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","\n","with torch.no_grad():  # 매우매우 중요! 테스트셋으로 학습하는 건 반칙입니다. 테스트셋으로 backprop을 하면 안 되지요.\n","    for data in test_loader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","        for i in range(4):\n","            label = labels[i]\n","            class_correct[label] += c[i].item()\n","            class_total[label] += 1\n","\n","for i in range(10):\n","    print('%5s 클래스의 정답률 : %2d %%' % (\n","        classes[i], 100 * class_correct[i] / class_total[i]))"]},{"cell_type":"markdown","metadata":{"id":"u7JncbdgFgvH"},"source":["# 딥러닝 끝!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[{"file_id":"https://github.com/1207koo/deep_iab/blob/master/01%2602_%5BCOLAB%5D_DataLoader_%26_Conv_Neural_Net_PyTorch.ipynb","timestamp":1680155743192}]}},"nbformat":4,"nbformat_minor":0}