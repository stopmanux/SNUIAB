{"cells":[{"cell_type":"markdown","metadata":{"id":"K9JzWDhUXkh6"},"source":["Copyright (C) 2023 Seoul National University\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","    http://www.apache.org/licenses/LICENSE-2.0\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\n","\n","## Colab 101\n","\n","Colab is a free Jupyter notebook environment by Google Research. Unlike AWS cluster (which is charged every hour it is up and running), you can run experiments on your own environment.\n","\n","## Colab Spark Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGKLcJu7HoB8"},"outputs":[],"source":["!apt-get update\n","!apt-get install openjdk-8-jdk-headless\n","!pip install -q findspark\n","!pip install pyspark==3.3.2\n","!curl -OL https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.tgz\n","!tar xzvf spark-3.3.2-bin-hadoop2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSL4Z8LJHp4F"},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop2\""]},{"cell_type":"markdown","metadata":{"id":"X-Pvn8rkXuHD"},"source":["## Wikipedia dataset sample\n","\n","This time we're not using HDFS to load the data. Sample data are loaded by Python code directly.\n","\n","The data has four fields: project, title, pageview count and size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTYHQ4OCU4LD"},"outputs":[],"source":["wikipedia_data_sample = [\"commons.m File:Gemblong.JPG 1 9717\"\n",",\"pl Beata_Tyszkiewicz 10 207378\"\n",",\"en Special:RecentChangesLinked/Roswell_(TV_series) 1 14617\"\n",",\"de Grafische_Benutzeroberfl%C3%A4che 1 22549\"\n",",\"en Simeon_I_of_Bulgaria 5 385793\"\n",",\"en Rainbow_Six_(novel) 8 122792\"\n",",\"es Pediatr%C3%ADa 5 73598\"\n",",\"sv Ett_uts%C3%B6kt_universum 1 9499\"\n",",\"en Video_game_content_rating_system 4 112324\"\n",",\"es Yuno_Gasai 2 55260\"\n",",\"en File:Georg_Wilhelm_Friedrich_Hegel00.jpg 1 43395\"\n",",\"en Anestia_ombrophanes 1 8881\"\n",",\"et Seitse 2 84874\"\n",",\"en And_I_Am_Telling_You_I%27m_Not_Going 4 85690\"\n",",\"he %D7%A4%D7%A8%D7%93%D7%99%D7%92%D7%9E%D7%94 1 13887\"\n",",\"zh File:Pictogram_voting_keep-green.svg 1 15106\"\n",",\"sv Special:Senaste_relaterade_%C3%A4ndringar/Homestead,_Florida 1 7677\"\n",",\"pt Categoria:Ambientes_de_desenvolvimento_integrado_livres 1 8151\"\n",",\"de.voy Plattensee 1 43748\"\n",",\"en Independent_Chip_Model 1 8938\"\n",",\"en Category:Toronto_Toros_players 2 0\"\n",",\"en Special:Export/Helsinki_Accords 1 19899\"\n",",\"xh Special:Contributions/Kpeterzell 1 5883\"\n",",\"nl 4_mei 1 0\"\n",",\"no Carlos_Keller_Rueff 5 87075\"\n",",\"en Special:Contributions/2.31.218.202 1 7402\"\n",",\"es Placa_Yangtze 1 10329\"\n",",\"de Datei:BSicon_uhKBHFe.svg 1 9786\"\n",",\"en Randolph_County,_Alabama 1 21431\"\n",",\"es S%C3%A9neca 3 70494\"\n",",\"en Tu_Bishvat 3 56438\"\n",",\"cs Radiohead 1 14325\"\n",",\"es Naturaleza_sangre 1 9286\"\n",",\"en Anatolia_(disambiguation) 1 7980\"\n",",\"pt Queima_de_suti%C3%A3s 1 8982\"\n",",\"pt Titanoboa_cerrejonensis 5 64540\"\n",",\"commons.m Category:People_of_Ireland 1 19278\"\n",",\"fi Matti_Inkinen 1 10138\"\n",",\"ja %E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Esfahan_(Iran)_Emam_Mosque.JPG 1 33168\"\n",",\"en Psicobloc 1 12739\"\n",",\"en Macael,_Spain 1 12658\"\n",",\"fa %DA%A9%D9%87%D8%AA%D9%88%DB%8C%D9%87 1 22855\"\n",",\"fr Sp%C3%A9cial:Pages_li%C3%A9es/Fichier:Wiki-ezokuroten5.jpg 1 21955\"\n",",\"nl Overleg_gebruiker:82.171.157.232 1 0\"\n",",\"en Thomas_%26_Mack_Center 2 41010\"\n",",\"en Warren_Beatty 49 2631986\"\n",",\"uz Auberville 1 11401\"]"]},{"cell_type":"markdown","metadata":{"id":"TG9kePIjX6A5"},"source":["## Spark RDD Transforms and Actions\n","\n","RDDs support two types of operations.\n","\n","**Transformations** create a new dataset from an existing one\n","\n","**Actions** return a value to the driver program after running a computation on the dataset.\n","\n","From now, we'll try several Spark RDD transforms using the sample wikipedia dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6n8go8ojHp_E"},"outputs":[],"source":["import findspark\n","from pyspark.sql import SparkSession\n","\n","findspark.init(os.environ[\"SPARK_HOME\"])\n","\n","# SparkSession is an entry point to programming Spark with the Dataset and DataFrame API\n","# SparkContext represents a connection to a Spark cluster, which can be used to create RDD and broadcast variables on the cluster.\n","\n","ss = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","sc = ss.sparkContext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZx1MpvJcdI1"},"outputs":[],"source":["# Parallelize the data and split into columns\n","lines = sc.parallelize(wikipedia_data_sample, 10)\n","\n","# Create a PythonRDD named 'columns'.\n","# A list of tuples, where each item in the tuple corresponds to space-split words in 'lines'\n","columns = lines.map(lambda line: tuple(line.split(\" \")))\n","# columns.collect()[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPcVG0I33j4J"},"outputs":[],"source":["# Element-Wise Transformation: Map Transform\n","\n","# Create a list of (project, count) tuples\n","project_count_tuples = columns.map(lambda column: (column[0], int(column[2])))\n","project_count_tuples.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SADcMkDj3OVc"},"outputs":[],"source":["# Element-Wise Transformation: Filter Transform\n","\n","# Filter project containing name 'de'\n","project_de_filtered = project_count_tuples.filter(lambda t: 'de' in t[0])\n","project_de_filtered.collect()"]},{"cell_type":"markdown","metadata":{"id":"ficWLfQesGfe"},"source":["## Quiz\n","Sample wikipedia data에서 project 의 count column 값이 5 이상인 경우만 filter 하시오.\n","- 결과값: project, count 로 구성된 tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK9-KpetsHlz"},"outputs":[],"source":["# Code here!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwCCbynGn3tf"},"outputs":[],"source":["# Element-Wise Transformation: sortByKey Transform\n","\n","# Sort key-value tuples by key in ascending order\n","project_sorted = project_count_tuples.sortByKey()\n","project_sorted.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMI4h6a-q4yX"},"outputs":[],"source":["# And in descending order\n","project_sorted_desc = project_count_tuples.sortByKey(ascending=False)\n","project_sorted_desc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhZ3zYpqWiy2"},"outputs":[],"source":["# Transformations on a single Pair RDD: ReduceByKey Transform\n","\n","# Compute the sum of pageview counts per project\n","# project_count_tuples.collect()[:5]\n","project_sum_tuples = project_count_tuples.reduceByKey(lambda a, b: a + b) \n","project_sum_tuples.collect()"]},{"cell_type":"markdown","metadata":{"id":"oblQYyRzsRfm"},"source":["## Quiz\n","Sample wikipedia data에서 project 별로 pageview count column 값을 곱하고 project가 'en'이 아닌 경우만 filter한 후, count 가 큰 순서대로 정렬하시오.\n","\n","- 결과값: project, count 로 구성된 tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ym_-NoQttBfn"},"outputs":[],"source":["# Code here!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVcZtTxoJvGm"},"outputs":[],"source":["# Transformations on two Pair RDDs: Join Transform\n","\n","# Declare additional wikipedia data.\n","# (recap: project, title (artist name), pageview count, size)\n","wikipedia_sample_artist = [\"en Lauv 49 2631986\"\n",",\"en SamSmith 1 12739\"\n",",\"en BTS 100 12658\"\n",",\"fa Eminem 1 22855\"\n",",\"en PostMalone 49 2631986\"]\n","\n","# And each artist's ranking information\n","artist_to_ranking = [\"SamSmith 1\"\n",",\"BTS 2\"\n",",\"Eminem 3\"\n",",\"PostMalone 4\"]\n","\n","# Parallelize both of them and split by spaces\n","lines2 = sc.parallelize(wikipedia_sample_artist, 5)\n","lines3 = sc.parallelize(artist_to_ranking, 4)\n","wikipedia_sample_artist_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n","artist_to_ranking_tuples = lines3.map(lambda line: tuple(line.split(\" \")))\n","\n","# Create a PythonRDD of (title, count) tuples\n","title_count_tuples = wikipedia_sample_artist_tuples.map(lambda column: (column[1], int(column[2])))\n","\n","# Join by title - the value would be (count, ranking) tuples\n","title_count_tuples.join(artist_to_ranking_tuples).collect()"]},{"cell_type":"markdown","metadata":{"id":"ALQ1yxKUw7ug"},"source":["## Quiz\n","Sample wikipedia data 로부터 (project, title) 로 된 PythonRDD 로 만든 다음, \n","아래의 project_to_projectid 와 project 로 join 하시오.\n","\n","- 결과값: key = project, value = (title, projectid) 인 tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96lAqesrw6iy"},"outputs":[],"source":["# Declare another sample data\n","project_to_projectid = [\"en 1\"\n",",\"fr 2\"\n",",\"de 3\"\n",",\"es 4\"]\n","\n","# Parallelize both of them and split by spaces\n","lines2 = sc.parallelize(wikipedia_data_sample, 10)\n","lines3 = sc.parallelize(project_to_projectid, 4)\n","wikipedia_sample_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n","proj_to_projid_tuples = lines3.map(lambda line: tuple(line.split(\" \")))\n","\n","# Code here!\n"]},{"cell_type":"markdown","metadata":{"id":"KgEM6fA2YaPz"},"source":["## SparkSQL\n","\n","Let's learn how to create SQL table by Spark DataFrame and execute SQL queries using Spark!\n","We'll use Wikipedia data above to create the table, and try some SQL operations using its four colums - project, title, count, size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z650qgjHTFLM"},"outputs":[],"source":["# Create a Spark DataFrame from wikipedia_data_sample (equivalent of an 'SQL table' in Spark)\n","df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n","\n","# Create a table view called \"WikipediaTable\"\n","df.createOrReplaceTempView(\"WikipediaTable\")\n","\n","df.show(df.count())\n","\n","# Run an SQL query that selects project equals to 'en' with count greater than or equal to 5\n","selected = ss.sql(\"SELECT project,title FROM WikipediaTable WHERE (project = 'en' AND count >= 5)\")\n","\n","# Print the results in this console (top 20 results will be shown)\n","selected.show(selected.count())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aw3J1GToUxl7"},"outputs":[],"source":["# Run an SQL query that orders projects by the number of titles each project has\n","selected = ss.sql(\"SELECT project, COUNT(title) AS num_of_title FROM WikipediaTable \\\n","GROUP BY project \\\n","ORDER BY num_of_title DESC\")\n","\n","# Print the results in this console (top 20 results will be shown)\n","selected.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_L1V1wTjDUD"},"outputs":[],"source":["# Get back to the two sample data\n","wikipedia_sample_artist = [\"en Lauv 49 2631986\"\n",",\"en SamSmith 1 12739\"\n",",\"en BTS 100 12658\"\n",",\"fa Eminem 1 22855\"\n",",\"en PostMalone 49 2631986\"]\n","\n","artist_to_ranking = [\"SamSmith 1\"\n",",\"BTS 2\"\n",",\"Eminem 3\"\n",",\"PostMalone 4\"]\n","\n","# Parallelize both of them and split by spaces\n","lines2 = sc.parallelize(wikipedia_sample_artist, 5)\n","lines3 = sc.parallelize(artist_to_ranking, 4)\n","\n","wikipedia_sample_artist_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n","artist_to_ranking_tuples = lines3.map(lambda line: tuple(line.split(\" \")))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iN-tZTf_RasG"},"outputs":[],"source":["# Create a Spark DataFrame from wikipedia_sample_artist and artist_to_ranking\n","wikipedia_sample_artist_df = ss.createDataFrame(wikipedia_sample_artist_tuples, ['project', 'title', 'count', 'size'])\n","artist_to_ranking_df = ss.createDataFrame(artist_to_ranking_tuples, ['title', 'ranking'])\n","\n","# Create a table view of them, called \"ArtistTable\" and \"RankingTable\"\n","wikipedia_sample_artist_df.createOrReplaceTempView(\"ArtistTable\")\n","artist_to_ranking_df.createOrReplaceTempView(\"RankingTable\")\n","\n","# Run an SQL query that joins the two tables.\n","# The result will show 'ranking' of RankingTable and 'title', 'count' of ArtistTable.\n","# Join will be performed on rows with common 'title' in both tables.\n","selected = ss.sql(\"SELECT RankingTable.ranking, ArtistTable.title, ArtistTable.count FROM ArtistTable \\\n","                   INNER JOIN RankingTable ON RankingTable.title=ArtistTable.title \\\n","                   ORDER BY RankingTable.ranking\")\n","\n","selected.show()"]},{"cell_type":"markdown","metadata":{"id":"FzKCclPWYmG-"},"source":["## Spark SQL Quiz 1. \n","'WikipediaTable'에서, 각 project 당 *count column 값의 총합이 10 이상인* (project, sum_of_count)를 구하시오\n","- 결과값: project, sum_of_count 2개의 column 을 갖는 테이블"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp9V8qXNY5CS"},"outputs":[],"source":["## Code here!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OLQZBBAiY_UN"},"source":["## Spark SQL Quiz 2.\n","ProjectGradeTable을 WikipediaTable과 join하여, grade가 'C'에 해당하는 project에 속하는 title들을 구하시오\n","- 결과값: title 1개의 column 을 갖는 테이블"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zm7cN8MuZNPk"},"outputs":[],"source":["cols = ['project', 'grade']\n","vals = [\n","     ('en', 'C'),\n","     ('he', 'A'),\n","     ('zh', 'B'),    \n","     ('no', 'A')\n","]\n","\n","project_grade = ss.createDataFrame(vals, cols)\n","project_grade.show()\n","project_grade.createOrReplaceTempView(\"ProjectGradeTable\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EnpkIUQbTKo"},"outputs":[],"source":["## Code here!\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}